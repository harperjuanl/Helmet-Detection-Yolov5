{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6545f4c-5e2d-48a6-88d0-4272ab9bf33c",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "```\n",
    "                                         1.2 Create an archive\n",
    "\n",
    "                                       ┌──────────────────────┐\n",
    "                                       │                      │\n",
    "                                       │   Torchserve Model   │\n",
    "                                       │       Archive        │\n",
    "                                       │                      │\n",
    "                                       │   ┌──────────────┐   │   ┌──────────────┐\n",
    "                                       │   │   model.py   │   │   │              │              1.3\n",
    "      1.1.1 from lab3   ───────────────┼─► ├──────────────┤   │   │  Torchserve  │              # workers\n",
    "                                       │   │ one_layer.pt │   │   │              │ ◄──────────  # batchsize\n",
    "                                       │   └──────────────┘   │   │    config    │              max batch delay\n",
    "                                       │                      │   │              │              etc.\n",
    "        preprocess  code               │   ┌──────────────┐   │   └──────┬───────┘\n",
    "1.1.2      call model     ─────────────┼─► │  handler.py  │   │          │\n",
    "        postprocess code               │   └──────────────┘   │          │\n",
    "                                       │                      │          │\n",
    "                                       └──────────┬───────────┘          │\n",
    "                                                  │                      │\n",
    "                                                  │                      │\n",
    "                                                  │                      │\n",
    "                                       ┌──────────▼──────────────────────▼───────┐\n",
    "                                       │                                         │    1.4 Upload to storage\n",
    "                                       │   Storage   ( MinIO / S3 / Url / PVC )  │\n",
    "                                       │                                         │\n",
    "                                       └────────────────────┬────────────────────┘\n",
    "                                                            │\n",
    "                                       ┌────────────────────▼────────────────────┐\n",
    "                                       │                                         │    2 Define KServe Yaml\n",
    "                                       │             KServe Predictor            │\n",
    "                                       │                                         │    3 Do some basic testing\n",
    "                                       │             ( scaling pods )            │\n",
    "                                       │                                         │    4 Autoscaling\n",
    "                                       └─────────────────────────────────────────┘\n",
    "                                                                                      5 Canary Rollout\n",
    "     \n",
    " ```\n",
    " \n",
    "The lab mainly covers:\n",
    "- PyTorch Serve: package PyTorch model with custom preprocess/postprocess functions\n",
    "- MinIO storage usage\n",
    "- KServe: basic, autoscaling, canary rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5610b8-6dce-41a9-8ab9-95cad9296bb5",
   "metadata": {},
   "source": [
    "## 1 PyTorch Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55886e",
   "metadata": {},
   "source": [
    "#### 1.1 Prepartion for Model Archiver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b721d1",
   "metadata": {},
   "source": [
    "Prepare 3 files:\n",
    "- pytorch_one_layer.pt: a serialized file (.pt or .pth) should be a checkpoint in case of torchscript and state_dict in case of eager mode.\n",
    "- model.py: a model file should contain the model architecture. This file is mandatory in case of eager mode models.\n",
    "- handler.py: codes for model initialization, pre-processing, post-processing, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ceb73",
   "metadata": {},
   "source": [
    "##### 1.1.1 pytorch_one_layer.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c0fec",
   "metadata": {},
   "source": [
    "I have already put it in `torchserve/pytorch_one_layer.pt`, which comes from [Lab3](../lab3_training.md):\n",
    "\n",
    "```python\n",
    "if RANK == 0:\n",
    "    print(\"saving model to\", args.dir)\n",
    "    os.makedirs(args.dir, exist_ok=True) \n",
    "    torch.save(model.state_dict(), os.path.join(args.dir, \"pytorch_one_layer.pt\"))\n",
    "```\n",
    "\n",
    "<span style=\"color:red\">If you are using JupyterLab in Kubeflow, remember to upload it to `torchserve/pytorch_one_layer.pt`</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0932219",
   "metadata": {},
   "source": [
    "##### 1.1.2 model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c186e",
   "metadata": {},
   "source": [
    "The pytorch_one_layer.pt does not contains model architecture, we need to provide model architecture definition with torchserve.\n",
    "\n",
    "Learn more about eager-mode vs torchscript here:\n",
    "https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa04231",
   "metadata": {},
   "source": [
    "Copy model architecture class `class Net(nn.Module)` from Lab3 to the cell below. \n",
    "\n",
    "Just run the cell and the code inside will be saved into `torchserve/model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2aca787f-7038-4e2d-a973-494ba27d5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p torchserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119fa2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting torchserve/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile torchserve/model.py\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f8e44",
   "metadata": {},
   "source": [
    "##### 1.1.3 Handler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fad86",
   "metadata": {},
   "source": [
    "What can handler.py do? (https://pytorch.org/serve/custom_service.html)\n",
    "\n",
    "- Initialize the model instance\n",
    "- Pre-process input data before it is sent to the model for inference or Captum explanations\n",
    "- Customize how the model is invoked for inference or explanations\n",
    "- Post-process output from the model before sending back a response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54349697",
   "metadata": {},
   "source": [
    "Implement `preprocess` and `postprocess` functions with the reference of `lab2` &  `lab3` \n",
    "- Preprocess: [Feature Extraction in lab2]\n",
    "- Postprocess: [PyTorch code in lab3]\n",
    "\n",
    "Just run the cell and the code inside will be saved into `torchserve/handler.py`\n",
    "\n",
    "1. torchserve web server: \n",
    "   1. combines multiple HTTP request into batches, forward batch requests to `Handler.py`\n",
    "      - input: \n",
    "         ```json\n",
    "         {\"email\": \"123\"}\n",
    "         ```\n",
    "         ```json\n",
    "         {\"email\": \"456\"}\n",
    "         ```\n",
    "      - output:\n",
    "         ```json\n",
    "         [{\"email\": \"123\"}, {\"email\": \"456\"}]\n",
    "         ```\n",
    "2. Handler.py\n",
    "   1. preprocess, convert list of dict into `torch tensor` for model inference\n",
    "      - input: output from torchserve webserver\n",
    "      - output:\n",
    "         ```python\n",
    "         [\n",
    "            [0, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 1],\n",
    "         ]\n",
    "         ```\n",
    "   2. PyTorch Model Inference,\n",
    "      - input: output from preprocess\n",
    "      - output:\n",
    "         ```python\n",
    "         [\n",
    "            [0.5, -0.3],\n",
    "            [0.3, 0.8],\n",
    "         ]\n",
    "         ```\n",
    "   3. Postprocess, input:\n",
    "      - input: output from PyTorch model\n",
    "      - output:\n",
    "         ```python\n",
    "         [\n",
    "            {'model_version': '1', 'prediction': 'ham'},\n",
    "            {'model_version': '1', 'prediction': 'spam'},\n",
    "         ]\n",
    "         ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ddb6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting torchserve/handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile torchserve/handler.py\n",
    "# custom handler file\n",
    "\n",
    "# model_handler.py\n",
    "\n",
    "\"\"\"\n",
    "ModelHandler defines a custom model handler.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "# BaseHandler:\n",
    "# https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py\n",
    "\n",
    "class ModelHandler(BaseHandler):\n",
    "    \"\"\"\n",
    "    A custom model handler implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocess(self, batch):\n",
    "        \"\"\"\n",
    "        Transform raw input into model input data.\n",
    "        :param batch: list of raw requests, should match batch size\n",
    "        :return: list of preprocessed model input data\n",
    "        \"\"\"\n",
    "        feature_list = []\n",
    "        logging.info(\"[preprocess] batch received:\")\n",
    "        logging.info(batch)\n",
    "        for email in batch:\n",
    "            # extract features from email\n",
    "            feature = []\n",
    "            # short text\n",
    "            short_text = len(email) < 500\n",
    "            feature.append(int(short_text))\n",
    "            # high frequency words\n",
    "            high_frequency_words = [\"body\", \"business\", \"html\", \"money\"]\n",
    "            for word in high_frequency_words:\n",
    "                contain_bool = word in email\n",
    "                feature.append(int(contain_bool))\n",
    "\n",
    "            feature_list.append(feature)\n",
    "\n",
    "        logging.info(\"Preprocess result:\")\n",
    "        logging.info(feature_list)\n",
    "        return torch.as_tensor(feature_list, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        \"\"\"\n",
    "        Return inference result.\n",
    "        :param inference_output: list of inference output\n",
    "        :return: list of predict results\n",
    "        \"\"\"\n",
    "        # Take output from network and post-process to desired format\n",
    "        logging.info(\"Logits from model:\")\n",
    "        logging.info(inference_output)\n",
    "\n",
    "        pred = inference_output.max(1)[1]\n",
    "        positive_dict = {\"version\": \"2\", \"prediction\": \"spam\"}\n",
    "        negative_dict = {\"version\": \"2\", \"prediction\": \"ham\"}\n",
    "        postprocess_result = list(map(\n",
    "                lambda x: positive_dict if x == 1 else negative_dict, \n",
    "                pred))\n",
    "\n",
    "        logging.info(\"Postprocess result:\")\n",
    "        logging.info(postprocess_result)\n",
    "        return postprocess_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82097fd3-c832-40da-90a6-8dd30f52ebc6",
   "metadata": {},
   "source": [
    "#### 1.2 Torchserve Model Archiver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8237c2-4dba-4428-9197-9140a393f9e9",
   "metadata": {},
   "source": [
    "It basically create a tar called `{model-name}.mar` from `model-file`, `serialized-file (*.pt)`, `handler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "4cfffb2d-3a0e-4642-912b-3a3bbe2e5c87",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch-model-archiver in /opt/conda/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: enum-compat in /opt/conda/lib/python3.8/site-packages (from torch-model-archiver) (0.0.3)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from torch-model-archiver) (0.18.2)\n",
      "create successfully\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $(dirname $0)/torchserve\n",
    "base_path=$(pwd)\n",
    "\n",
    "mkdir -p $base_path/model-store && cd $base_path/model-store &&\n",
    "if [ -f $base_path/model-store/helmet_detection.mar ]; then\n",
    "    rm $base_path/model-store/helmet_detection.mar\n",
    "fi\n",
    "\n",
    "pip install torch-model-archiver -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "\n",
    "torch-model-archiver --model-name helmet_detection \\\n",
    "--version 0.1 --serialized-file $base_path/helmet.torchscript.pt \\\n",
    "--handler $base_path/torchserve_handler.py \\\n",
    "--extra-files $base_path/index_to_name.json,$base_path/torchserve_handler.py\n",
    "\n",
    "\n",
    "echo \"create successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53742313-cfe9-4e29-b71e-9d7f92cfb606",
   "metadata": {},
   "source": [
    "#### 1.3 create torchserve config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eca6a9",
   "metadata": {},
   "source": [
    "Feel free to change the parameters:\n",
    "\n",
    "- minWorkers: the minimum number of workers of a model\n",
    "- maxWorkers: the maximum number of workers of a model\n",
    "- batchSize: the batch size of a model\n",
    "- maxBatchDelay: the maximum dalay in msec of a batch of a model\n",
    "- responseTimeout: the timeout in msec of a model's response\n",
    "- defaultVersion: the default version of a model\n",
    "- marName: the mar file name of a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a96fba-15da-44e1-bd4c-877dba5dcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p torchserve/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "c55089cd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting torchserve/config/config.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile torchserve/config/config.properties\n",
    "\n",
    "inference_address=http://0.0.0.0:8085\n",
    "management_address=http://0.0.0.0:8081\n",
    "metrics_address=http://0.0.0.0:8082\n",
    "grpc_inference_port=7070\n",
    "grpc_management_port=7071\n",
    "enable_metrics_api=true\n",
    "metrics_format=prometheus\n",
    "number_of_netty_threads=4\n",
    "job_queue_size=10\n",
    "enable_envvars_config=true\n",
    "install_py_dep_per_model=true\n",
    "model_store=/home/model-server/torchserve_mar/helmet_detection/model-store\n",
    "model_snapshot={\"name\":\"startup.cfg\",\"modelCount\":1,\"models\":{\"helmet_detection\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"helmet_detection.mar\",\"minWorkers\":1,\"maxWorkers\":5,\"batchSize\":4,\"maxBatchDelay\":100,\"responseTimeout\":120}}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99e743-1903-4b7b-972b-06bfe995a12b",
   "metadata": {},
   "source": [
    "#### 1.4 Upload to MinIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49499526-d671-489a-8b9e-2522330c97f3",
   "metadata": {},
   "source": [
    "If you already have the minio storage, you can directly follow the next steps. If not, we also provide a standalone minio deployment guide on the kubernetes clusters.\n",
    "\n",
    "You can use the files from here [https://github.com/xujinheng/kubeflow-manifests/tree/main/website/content/en/docs/kubeflow-tutorial/lab4_minio_deploy], and apply in your clusters.\n",
    "\n",
    "`kubectl apply -f minio-standalone-pvc.yml` \n",
    "\n",
    "`kubectl apply -f minio-standalone-service.yml`\n",
    "\n",
    "`kubectl apply -f minio-standalone-deployment.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc43bf8",
   "metadata": {},
   "source": [
    "This step uploads `torchserve/model-store`, `torchserve/config` to MinIO buckets\n",
    "\n",
    "You need to find the MINIO\n",
    "- `endpoint_url`\n",
    "- `key_id`\n",
    "- `access_key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644fb1ea-3364-48a0-a312-535ceb9b7048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting boto3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/29/17/8dd2d2c231cdfed1b24e31e49c628b8490c2846fe3116ced9d2fa73de0aa/boto3-1.25.5-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.29.0,>=1.28.5\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/08/7c/5672539c66ab305e385fcb578b395feec894e3277f35843a1e4c94259fb3/botocore-1.28.5-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/31/b4/b9b800c45527aadd64d5b442f9b932b00648617eb5d63d2c7a6587b7cafc/jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5e/c6/af903b5fab3f9b5b1e883f49a770066314c6dcceb589cf938d48c89556c1/s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.29.0,>=1.28.5->boto3) (1.26.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.29.0,>=1.28.5->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.0,>=1.28.5->boto3) (1.16.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.25.5 botocore-1.28.5 jmespath-1.0.1 s3transfer-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1e69a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import boto3\n",
    "\n",
    "os.environ[\"AWS_ENDPOINT_URL\"] = \"http://10.117.233.16:9000\"\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                    endpoint_url=os.getenv(\"AWS_ENDPOINT_URL\"),\n",
    "                    verify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b1b511d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current buckets in s3:\n",
      "[s3.Bucket(name='helmet-detection-bucket'), s3.Bucket(name='juanl-bucket'), s3.Bucket(name='xujinheng-bucket')]\n"
     ]
    }
   ],
   "source": [
    "print(\"current buckets in s3:\")\n",
    "print(list(s3.buckets.all()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6acc33f8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Bucket(name='juanl-bucket')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name='juanl-bucket'\n",
    "s3.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1b2f6",
   "metadata": {},
   "source": [
    "Upload files to your bucket_name, and you can also specify `bucket_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "efb5c40e-808e-49cc-9db5-070fc2b51841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helmet_detection/config/config.properties\n",
      "helmet_detection/model-store/helmet_detection.mar\n"
     ]
    }
   ],
   "source": [
    "curr_path = os.getcwd()\n",
    "base_path = os.path.join(curr_path, \"torchserve\")\n",
    "\n",
    "bucket_path = \"helmet_detection\"\n",
    "\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "# upload\n",
    "bucket.upload_file(os.path.join(base_path, \"model-store\", \"helmet_detection.mar\"),\n",
    "                   os.path.join(bucket_path, \"model-store/helmet_detection.mar\"))\n",
    "bucket.upload_file(os.path.join(base_path, \"config\", \"config.properties\"), \n",
    "                   os.path.join(bucket_path, \"config/config.properties\"))\n",
    "\n",
    "# check files \n",
    "for obj in bucket.objects.filter(Prefix=bucket_path):\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3fe861-c578-4f37-9f55-7c00b9b28b82",
   "metadata": {},
   "source": [
    "## 2 KServe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fbbc31-87a5-43f3-b7c6-5254091b8783",
   "metadata": {},
   "source": [
    "#### 2.1 Create Minio service account && secret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f14c8-dab6-499c-b16b-0d1aea473ad7",
   "metadata": {},
   "source": [
    "- You will also need to specify the `s3-endpoint`, `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` here\n",
    "- If you are using default user `user@exampe.com/12341234`, please also set a different name for all the <span style=\"color:red\">metadata: name</span> in the yaml file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "257bbe98-3840-4993-aa97-0b8802b1baf1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/minio-s3-secret-user configured\n",
      "serviceaccount/minio-service-account-user configured\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat << EOF | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: minio-s3-secret-user\n",
    "  annotations:\n",
    "     serving.kserve.io/s3-endpoint: \"10.117.233.16:9000\" # replace with your s3 endpoint e.g minio-service.kubeflow:9000\n",
    "     serving.kserve.io/s3-usehttps: \"0\" # by default 1, if testing with minio you can set to 0\n",
    "     serving.kserve.io/s3-region: \"us-east-2\"\n",
    "     serving.kserve.io/s3-useanoncredential: \"false\" # omitting this is the same as false, if true will ignore provided credential and use anonymous credentials\n",
    "type: Opaque\n",
    "stringData: # use \"stringData\" for raw credential string or \"data\" for base64 encoded string\n",
    "  AWS_ACCESS_KEY_ID: minioadmin\n",
    "  AWS_SECRET_ACCESS_KEY: minioadmin\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: minio-service-account-user\n",
    "secrets:\n",
    "- name: minio-s3-secret-user\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5503d63-9294-4974-8c81-822fa790d5c5",
   "metadata": {},
   "source": [
    "#### 2.2 Create InferenceService from MinIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81556391",
   "metadata": {},
   "source": [
    "- Set `storageUri` to your `bucket_name/bucket_path`\n",
    "- You may also need to change `metadata: name` and `serviceAccountName` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "8bb67b2a-e446-46d3-9851-f12f51c49500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/helmet-detection-serving created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat << EOF | kubectl apply -f -\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"helmet-detection-serving\"\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: minio-service-account-user\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: pytorch\n",
    "      storageUri: \"s3://juanl-bucket/helmet_detection\"\n",
    "      resources:\n",
    "          requests:\n",
    "            cpu: 50m\n",
    "            memory: 200Mi\n",
    "          limits:\n",
    "            cpu: 100m\n",
    "            memory: 500Mi\n",
    "          # limits:\n",
    "          #   nvidia.com/gpu: \"1\"   # for inference service on GPU\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb8883",
   "metadata": {},
   "source": [
    "#### 2.3 Kubeflow UI\n",
    "\n",
    "Check model logs at [Kubeflow UI -> Models](/models/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01972d2-becc-45d9-8439-6de84d26caf7",
   "metadata": {},
   "source": [
    "## 3 Test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec065802",
   "metadata": {},
   "source": [
    "#### 3.1 Define a Test_bot for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "212a5487-b3a6-4765-96ef-704fd43c586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting multiprocess\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/13/95/8b875a678c6f9db81809dd5d6032e9f8628426e37f6aa6b7d404ba582de1/multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m482.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill>=0.3.6\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/be/e3/a84bf2e561beed15813080d693b4b27573262433fced9c1d1fea59e60553/dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: dill, multiprocess\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "Successfully installed dill-0.3.6 multiprocess-0.70.14\n"
     ]
    }
   ],
   "source": [
    "!pip install multiprocess -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d1fe8776-a5b2-4dd5-b364-0d29df632f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import multiprocess as mp\n",
    "import io\n",
    "import base64\n",
    "import PIL.Image as Image\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "class Test_bot():\n",
    "    def __init__(self, uri, model, host, session):\n",
    "        self.uri = uri\n",
    "        self.model = model\n",
    "        self.host = host\n",
    "        self.session = session\n",
    "        self.headers = {'Host': self.host, 'Content-Type': \"image/jpeg\", 'Cookie': \"authservice_session=\" + self.session}\n",
    "        self.img = './1.jpg'\n",
    "    \n",
    "    def update_uri(self, uri):\n",
    "        self.uri = uri\n",
    "        \n",
    "    def update_model(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def update_host(self, host):\n",
    "        self.host = host\n",
    "        self.update_headers()\n",
    "        \n",
    "    def update_session(self, session):\n",
    "        self.session = session\n",
    "        self.update_headers()\n",
    "        \n",
    "    def update_headers(self):\n",
    "        self.headers = {'Host': self.host, 'Content-Type': \"image/jpeg\", 'Cookie': \"authservice_session=\" + self.session}\n",
    "        \n",
    "    def get_data(self, x):\n",
    "        if x:\n",
    "            payload = x\n",
    "        else: \n",
    "            payload = self.img\n",
    "        with open(payload, \"rb\") as image:  \n",
    "            f = image.read()\n",
    "            image_data = base64.b64encode(f).decode('utf-8')    \n",
    "\n",
    "        return json.dumps({'instances': [image_data]})\n",
    "\n",
    "    \n",
    "    def predict(self, x=None):\n",
    "        uri = self.uri + '/v1/models/' + self.model + ':predict'\n",
    "        response = requests.request(\"POST\", uri, headers=self.headers, data=self.get_data(x))\n",
    "        return response.text\n",
    "    \n",
    "        \n",
    "    def readiness(self):\n",
    "        # uri = self.uri + '/v1/models/' + self.model\n",
    "        uri = self.uri + '/v1/models/' + self.model\n",
    "        response = requests.get(uri, headers = self.headers, timeout=5)\n",
    "        return response.text\n",
    "\n",
    "    \n",
    "    def explain(self, x=None):\n",
    "        uri = self.uri + '/v1/models/' + self.model + ':explain'\n",
    "        response = requests.post(uri, data=self.get_data(x), headers = self.headers, timeout=10)\n",
    "        return response.text\n",
    "    \n",
    "    def concurrent_predict(self, num=10):\n",
    "        print(\"fire \" + str(num) + \" requests to \" + self.host)\n",
    "        with mp.Pool() as pool:\n",
    "            responses = pool.map(self.predict, range(num))\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1311d0b7",
   "metadata": {},
   "source": [
    "#### 3.2 Determine host and session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7aadc6",
   "metadata": {},
   "source": [
    "Run the following cell to get `host`, which will be set to the headers in our request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "cfb136db-76ac-4a9b-81a2-d49529ec2474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helmet-detection-serving.kubeflow-user-example-com.example.com\n"
     ]
    }
   ],
   "source": [
    "!kubectl get inferenceservice helmet-detection-serving -o jsonpath='{.status.url}' | cut -d \"/\" -f 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709d26a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Use your web browser to login to Kubeflow, and get `Cookies: authservice_session` (Chrome: Developer Tools -> Applications -> Cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "5079b4e7-ee8f-4388-a130-f5484d7bbf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"helmet_detection\", \"ready\": true}\n",
      "uri:  http://10.117.233.8/v1/models/helmet_detection:predict\n",
      "{'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'}\n",
      "{\"predictions\": [[{\"x1\": 0.16830308735370636, \"y1\": 0.36698096990585327, \"x2\": 0.3356268107891083, \"y2\": 0.5662754774093628, \"confidence\": 0.9418922662734985, \"class\": \"person\"}, {\"x1\": -0.0003847241459880024, \"y1\": 0.26973700523376465, \"x2\": 0.11975767463445663, \"y2\": 0.5021408796310425, \"confidence\": 0.9287041425704956, \"class\": \"person\"}, {\"x1\": 0.31550225615501404, \"y1\": 0.27130556106567383, \"x2\": 0.4195330739021301, \"y2\": 0.4244980812072754, \"confidence\": 0.922441303730011, \"class\": \"person\"}, {\"x1\": 0.8000055551528931, \"y1\": 0.36035841703414917, \"x2\": 0.8742902874946594, \"y2\": 0.4628569483757019, \"confidence\": 0.9012498259544373, \"class\": \"person\"}, {\"x1\": 0.44192060828208923, \"y1\": 0.39776062965393066, \"x2\": 0.5190550088882446, \"y2\": 0.48923078179359436, \"confidence\": 0.8915992379188538, \"class\": \"person\"}, {\"x1\": 0.9677120447158813, \"y1\": 0.4071219861507416, \"x2\": 0.9998529553413391, \"y2\": 0.5111278891563416, \"confidence\": 0.8755874633789062, \"class\": \"person\"}, {\"x1\": 0.5246236324310303, \"y1\": 0.39841872453689575, \"x2\": 0.5718141794204712, \"y2\": 0.4656790792942047, \"confidence\": 0.8437986373901367, \"class\": \"hat\"}, {\"x1\": 0.6443458795547485, \"y1\": 0.2609959840774536, \"x2\": 0.7564457654953003, \"y2\": 0.443324476480484, \"confidence\": 0.8425442576408386, \"class\": \"person\"}, {\"x1\": 0.6181862950325012, \"y1\": 0.36550217866897583, \"x2\": 0.6777603030204773, \"y2\": 0.4528803825378418, \"confidence\": 0.7514654397964478, \"class\": \"person\"}]]}\n"
     ]
    }
   ],
   "source": [
    "# replace it with the url you used to access Kubeflow\n",
    "bot = Test_bot(uri='http://10.117.233.8',\n",
    "               model='helmet_detection',\n",
    "               # replace it with what is printed above\n",
    "               host='helmet-detection-serving.kubeflow-user-example-com.example.com',\n",
    "               # replace it\n",
    "               session='MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp')\n",
    "\n",
    "print(bot.readiness()) \n",
    "print(bot.predict('./1.jpg'))\n",
    "# We didn't implement model explainer, so this result will be 500: Internal Server Error\n",
    "# https://kserve.github.io/website/0.8/modelserving/explainer/explainer/\n",
    "# print(bot.explain(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e69e12-085d-49d8-ae31-aacda5a66d36",
   "metadata": {},
   "source": [
    "## 4 Autoscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc6513",
   "metadata": {},
   "source": [
    "- Knative Pod Autoscaler (KPA)\n",
    "  - Part of the Knative Serving core and enabled by default once Knative Serving is installed.\n",
    "  - Supports scale to zero functionality.\n",
    "  - Does not support CPU-based autoscaling.\n",
    "  \n",
    "- Horizontal Pod Autoscaler (HPA)\n",
    "  - Not part of the Knative Serving core, and must be enabled after Knative Serving installation.\n",
    "  - Does not support scale to zero functionality.\n",
    "  - Supports CPU-based autoscaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f7acb-3c77-4abf-ad9f-bdbc310b532e",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">If you use CPU-based autotscaling, ake sure HPA is installed before move on </span> (check by `kubectl get deploy autoscaler-hpa -n knative-serving`), will need to install it from https://github.com/knative/serving/releases/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19394e",
   "metadata": {},
   "source": [
    "Add autoscaling tag to the InferenceService and apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "74b38359-7824-450f-bb89-f5f1252ab57b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/helmet-detection-serving configured\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat << EOF | kubectl apply -f -\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: helmet-detection-serving\n",
    "  annotations:\n",
    "    autoscaling.knative.dev/class: hpa.autoscaling.knative.dev\n",
    "    # see available tags: https://knative.dev/docs/serving/autoscaling/autoscaling-targets/\n",
    "    autoscaling.knative.dev/max-scale: \"3\"\n",
    "    # HPA: specifies the CPU percentage target (default \"80\"). \n",
    "    # KPA: Target x requests in-flight per pod.\n",
    "    autoscaling.knative.dev/target: \"80\"  \n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: minio-service-account-user\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: pytorch\n",
    "      storageUri: \"s3://juanl-bucket/helmet_detection\"\n",
    "      resources:\n",
    "          requests:\n",
    "            cpu: 50m\n",
    "            memory: 200Mi\n",
    "          limits:\n",
    "            cpu: 100m\n",
    "            memory: 500Mi\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e561285",
   "metadata": {},
   "source": [
    "Check the number of pods. It takes a while before the one deployment get replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "6916d61d-3521-4cc6-b461-7fbd19594e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                              READY   STATUS                  RESTARTS   AGE\n",
      "bitfusion-notebook-01-0                                           2/2     Running                 0          61d\n",
      "helmet-detection-0                                                2/2     Running                 0          14d\n",
      "helmet-detection-deployment-565dbfcffd-d6czf                      2/2     Running                 0          12d\n",
      "helmet-detection-serving-predictor-default-00001-deploymen92b59   1/3     Terminating             0          13h\n",
      "helmet-detection-serving-predictor-default-00002-deploymen8dmbp   0/3     Init:CrashLoopBackOff   3          74s\n",
      "helmet-detection-serving-predictor-default-00003-deploymenmkngh   3/3     Running                 0          4m4s\n",
      "ml-pipeline-ui-artifact-7cd897c59f-kzlfs                          2/2     Running                 0          64d\n",
      "ml-pipeline-visualizationserver-795f7db965-gzjsm                  2/2     Running                 0          64d\n",
      "model-serving-test-0                                              2/2     Running                 0          15d\n",
      "sklearn-iris-predictor-default-00001-deployment-5484f4d57-ld2fr   3/3     Running                 0          15d\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4770bcd",
   "metadata": {},
   "source": [
    "Adjust num of concurrent predict requests, fire it, let the the number of pods scale up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f94e9-1d96-4a8d-a11f-79c60dc02cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fire 100 requests to helmet-detection-serving.kubeflow-user-example-com.example.com\n",
      "uri:  uri: http://10.117.233.8/v1/models/helmet_detection:predict\n",
      " uri: {'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'} \n",
      "http://10.117.233.8/v1/models/helmet_detection:predicthttp://10.117.233.8/v1/models/helmet_detection:predicturi: \n",
      " http://10.117.233.8/v1/models/helmet_detection:predict{'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'}\n",
      "\n",
      "{'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'}\n",
      "{'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'}\n",
      "uri: \n",
      " uri: uri:   http://10.117.233.8/v1/models/helmet_detection:predicthttp://10.117.233.8/v1/models/helmet_detection:predict\n",
      "http://10.117.233.8/v1/models/helmet_detection:predict{'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'}\n",
      "\n",
      "{'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'}\n",
      "uri:  uri: http://10.117.233.8/v1/models/helmet_detection:predict \n",
      "\n",
      "http://10.117.233.8/v1/models/helmet_detection:predict{'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'}\n",
      "\n",
      "{'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'}\n",
      "{'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'}\n",
      "uri:  http://10.117.233.8/v1/models/helmet_detection:predict\n",
      "{'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'}\n",
      "uri:  http://10.117.233.8/v1/models/helmet_detection:predict\n",
      "{'Host': 'helmet-detection-serving.kubeflow-user-example-com.example.com', 'Content-Type': 'image/jpeg', 'Cookie': 'authservice_session=MTY2NzM4MzA3NnxOd3dBTkZOT1dVRTJVRVZWVUVaVlRFeEdSVFpLVmxwRk1rRlhRMHhIUVRKR05sQklTVmswTmxOTVdsaFdTRmxNUkV0TFJqSkxOVkU9fPcleb6sw1pZHcTLy5HMQRLssZ7PP_nQkhOTVGV7MBEp'}\n"
     ]
    }
   ],
   "source": [
    "responses = bot.concurrent_predict(num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb241b7f",
   "metadata": {},
   "source": [
    "Check the number of pods again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7688b5-5766-4e91-8c65-c041a2a434be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0704f",
   "metadata": {},
   "source": [
    "## 6 More\n",
    "Explore the Kserve 0.8 docs here https://kserve.github.io/website/0.8/modelserving/control_plane/\n",
    "\n",
    "(note that the version we use is KServe 0.6.1)\n",
    "\n",
    "- Multi Model Serving\n",
    "- Transformers\n",
    "- Model Explainability\n",
    "- Model Monitoring\n",
    "- Payload Logging\n",
    "- etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d8fcee15318561c421fd029289997adde12df0a6deb462b29cf55fa6694a6d5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
